{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3514d0cd-7174-4f57-8af2-76dc9ea9cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 517\n",
      "Vocabulary size: 11724\n",
      "Matrix shape (docs × terms): 517 × 11724\n",
      "\n",
      "Doc 0 top TF–IDF terms (term, tfidf, idf):\n",
      "  flyer                0.7739  (idf=3.9178)\n",
      "  algebrafactsheet     0.1996  (idf=5.0528)\n",
      "  bcs                  0.1996  (idf=5.0528)\n",
      "  egk                  0.1996  (idf=5.0528)\n",
      "  primescircl          0.1917  (idf=4.8521)\n",
      "  crowdmath            0.1722  (idf=4.3596)\n",
      "  ethnic               0.1722  (idf=4.3596)\n",
      "  chines               0.1534  (idf=3.8827)\n",
      "  vti                  0.1471  (idf=3.7236)\n",
      "  cnf                  0.1460  (idf=3.6946)\n",
      "\n",
      "Doc 10 top TF–IDF terms (term, tfidf, idf):\n",
      "  entpro               0.3852  (idf=3.1391)\n",
      "  sheth                0.3046  (idf=3.7236)\n",
      "  confer               0.2407  (idf=1.9617)\n",
      "  miniconfer           0.2233  (idf=5.4582)\n",
      "  zhangv               0.2024  (idf=4.9474)\n",
      "  hase                 0.1858  (idf=4.5419)\n",
      "  mural                0.1858  (idf=4.5419)\n",
      "  pierson              0.1831  (idf=4.4774)\n",
      "  yeiser               0.1831  (idf=4.4774)\n",
      "  rohatgi              0.1807  (idf=4.4168)\n",
      "\n",
      "Doc 100 top TF–IDF terms (term, tfidf, idf):\n",
      "  entpro               0.3852  (idf=3.1391)\n",
      "  sheth                0.3046  (idf=3.7236)\n",
      "  confer               0.2407  (idf=1.9617)\n",
      "  miniconfer           0.2233  (idf=5.4582)\n",
      "  zhangv               0.2024  (idf=4.9474)\n",
      "  hase                 0.1858  (idf=4.5419)\n",
      "  mural                0.1858  (idf=4.5419)\n",
      "  pierson              0.1831  (idf=4.4774)\n",
      "  yeiser               0.1831  (idf=4.4774)\n",
      "  rohatgi              0.1807  (idf=4.4168)\n",
      "\n",
      "\n",
      "Saved outputs to: C:\\Users\\Owner\\outputs\n",
      "Artifacts:\n",
      " - C:\\Users\\Owner\\outputs\\cleaned_corpus.txt\n",
      " - C:\\Users\\Owner\\outputs\\vocabulary.json\n",
      " - C:\\Users\\Owner\\outputs\\vocabulary.csv\n",
      " - C:\\Users\\Owner\\outputs\\bow_counts.npz\n",
      " - C:\\Users\\Owner\\outputs\\tfidf_matrix.npz\n",
      " - C:\\Users\\Owner\\outputs\\count_vectorizer.joblib\n",
      " - C:\\Users\\Owner\\outputs\\tfidf_transformer.joblib\n",
      " - C:\\Users\\Owner\\outputs\\summary.txt\n",
      " - C:\\Users\\Owner\\outputs\\cleaned_corpus_with_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# === IS688 - Milestone 2: Preprocessing → BoW → TF–IDF (Upgraded) ===\n",
    "# Adds:\n",
    "# - joblib persistence for the fitted CountVectorizer & TfidfTransformer\n",
    "# - optional lemmatization (fallback-safe)\n",
    "# - vocabulary.csv (teacher-friendly)\n",
    "# - richer summary including IDF\n",
    "# - optional max_features to stabilize vocab size across runs\n",
    "\n",
    "import os, json, re, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import text as sk_text\n",
    "import joblib  # <-- persistence\n",
    "\n",
    "# ---------- Config ----------\n",
    "INPUT_PATH = r\"C:\\Users\\Owner\\Downloads\\records.jsonl\"  # <-- change if needed\n",
    "TEXT_COL   = \"text\"                                     # column that has the raw text\n",
    "OUT_DIR    = Path(\"./outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Toggle if you prefer lemmatization instead of stemming; falls back gracefully if resources missing\n",
    "USE_LEMMATIZATION = False\n",
    "\n",
    "# Vectorizer stability (optional): cap vocab size if you want consistent dimensions across runs\n",
    "MAX_FEATURES = None  # e.g., 10000 or None to disable\n",
    "\n",
    "# ---------- Load ----------\n",
    "if INPUT_PATH.lower().endswith((\".jsonl\", \".ndjson\")):\n",
    "    df = pd.read_json(INPUT_PATH, lines=True)\n",
    "else:\n",
    "    df = pd.read_json(INPUT_PATH)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise ValueError(f\"Column '{TEXT_COL}' not found. Available: {list(df.columns)}\")\n",
    "\n",
    "raw_docs = df[TEXT_COL].astype(str).fillna(\"\")\n",
    "\n",
    "# ---------- Preprocessing helpers ----------\n",
    "extra_sw = {\n",
    "    \"index\", \"research\", \"highschool\", \"primes\", \"materials\",\n",
    "    \"pdf\", \"html\", \"www\", \"http\", \"https\"\n",
    "}\n",
    "STOPWORDS = sk_text.ENGLISH_STOP_WORDS.union(extra_sw)\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[a-z]+\")\n",
    "\n",
    "def maybe_normalize(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize OR stem OR noop, depending on availability and setting.\n",
    "    \"\"\"\n",
    "    if USE_LEMMATIZATION:\n",
    "        try:\n",
    "            import nltk\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            # If wordnet isn't available, this will still run without crashing, just less effective\n",
    "            # nltk.download('wordnet'); nltk.download('omw-1.4')\n",
    "            lem = WordNetLemmatizer()\n",
    "            return [lem.lemmatize(t) for t in tokens]\n",
    "        except Exception:\n",
    "            pass  # fall through to stemming or noop\n",
    "\n",
    "    # Try stemming next\n",
    "    try:\n",
    "        from nltk.stem import SnowballStemmer\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        return [stemmer.stem(t) for t in tokens]\n",
    "    except Exception:\n",
    "        return tokens  # fallback: noop\n",
    "\n",
    "def preprocess_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    tokens = TOKEN_RE.findall(s)                     # keep letters only\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 2]\n",
    "    tokens = maybe_normalize(tokens)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ---------- Build cleaned corpus ----------\n",
    "cleaned = [preprocess_text(doc) for doc in raw_docs]\n",
    "\n",
    "with open(OUT_DIR / \"cleaned_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in cleaned:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# ---------- Bag-of-Words (CountVectorizer) ----------\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=str.split,     # we've already tokenized via spaces\n",
    "    preprocessor=None,\n",
    "    lowercase=False,         # already lowercased\n",
    "    min_df=2,                # filter very rare terms\n",
    "    max_df=0.95,             # drop overly common terms\n",
    "    max_features=MAX_FEATURES\n",
    ")\n",
    "X_counts = vectorizer.fit_transform(cleaned)  # CSR sparse\n",
    "\n",
    "# Save vocabulary (term -> column index) as json and csv\n",
    "vocab = vectorizer.vocabulary_  # dict: term -> col\n",
    "with open(OUT_DIR / \"vocabulary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "vocab_items = sorted(vocab.items(), key=lambda x: x[1])  # sort by column index\n",
    "pd.DataFrame(vocab_items, columns=[\"term\", \"col_index\"]).to_csv(\n",
    "    OUT_DIR / \"vocabulary.csv\", index=False, encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# Persist counts matrix\n",
    "sparse.save_npz(OUT_DIR / \"bow_counts.npz\", X_counts)\n",
    "\n",
    "# Persist the fitted CountVectorizer (for Milestone 3 reuse)\n",
    "joblib.dump(vectorizer, OUT_DIR / \"count_vectorizer.joblib\")\n",
    "\n",
    "# ---------- TF–IDF ----------\n",
    "tfidf = TfidfTransformer(norm=\"l2\", use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "X_tfidf = tfidf.fit_transform(X_counts)\n",
    "\n",
    "# Persist TF–IDF matrix and the fitted transformer\n",
    "sparse.save_npz(OUT_DIR / \"tfidf_matrix.npz\", X_tfidf)\n",
    "joblib.dump(tfidf, OUT_DIR / \"tfidf_transformer.joblib\")\n",
    "\n",
    "# ---------- Summary report ----------\n",
    "vocab_size = len(vocab)\n",
    "n_docs, n_terms = X_counts.shape\n",
    "idf = tfidf.idf_  # array aligned to columns\n",
    "\n",
    "# Build reverse vocab for quick lookup\n",
    "rev_vocab = {j: t for t, j in vocab.items()}\n",
    "\n",
    "def top_tfidf_terms_with_idf(row_csr, k=10):\n",
    "    if row_csr.nnz == 0:\n",
    "        return []\n",
    "    idx = row_csr.indices\n",
    "    vals = row_csr.data\n",
    "    top = np.argsort(vals)[-k:][::-1]\n",
    "    return [(rev_vocab[idx[i]], float(vals[i]), float(idf[idx[i]])) for i in top]\n",
    "\n",
    "sample_docs = [0, min(10, n_docs-1), min(100, n_docs-1)]\n",
    "lines = []\n",
    "lines.append(f\"Documents: {n_docs}\")\n",
    "lines.append(f\"Vocabulary size: {vocab_size}\")\n",
    "lines.append(f\"Matrix shape (docs × terms): {X_tfidf.shape[0]} × {X_tfidf.shape[1]}\")\n",
    "lines.append(\"\")\n",
    "\n",
    "for d in sample_docs:\n",
    "    tops = top_tfidf_terms_with_idf(X_tfidf[d], k=10)\n",
    "    lines.append(f\"Doc {d} top TF–IDF terms (term, tfidf, idf):\")\n",
    "    for t, tfidf_w, idf_w in tops:\n",
    "        lines.append(f\"  {t:20s} {tfidf_w:.4f}  (idf={idf_w:.4f})\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "with open(OUT_DIR / \"summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "\n",
    "# ---------- Optional: also save a compact CSV for instructor ----------\n",
    "df_out = pd.DataFrame({\n",
    "    \"url\": df.get(\"url\", pd.Series([None]*len(df))),\n",
    "    \"type\": df.get(\"type\", pd.Series([None]*len(df))),\n",
    "    \"math_symbol_count\": df.get(\"math_symbol_count\", pd.Series([None]*len(df))),\n",
    "    \"cleaned_text\": cleaned\n",
    "})\n",
    "df_out.to_csv(OUT_DIR / \"cleaned_corpus_with_meta.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nSaved outputs to: {OUT_DIR.resolve()}\")\n",
    "print(\"Artifacts:\")\n",
    "for p in [\n",
    "    \"cleaned_corpus.txt\",\n",
    "    \"vocabulary.json\",\n",
    "    \"vocabulary.csv\",\n",
    "    \"bow_counts.npz\",\n",
    "    \"tfidf_matrix.npz\",\n",
    "    \"count_vectorizer.joblib\",\n",
    "    \"tfidf_transformer.joblib\",\n",
    "    \"summary.txt\",\n",
    "    \"cleaned_corpus_with_meta.csv\",\n",
    "]:\n",
    "    print(\" -\", (OUT_DIR / p).resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d556f8-4779-4a09-9074-2d39ed8d71fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
